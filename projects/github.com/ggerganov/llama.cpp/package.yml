distributable:
  url: https://github.com/ggerganov/llama.cpp/archive/refs/tags/b{{version.raw}}.tar.gz
  strip-components: 1

versions:
  github: ggerganov/llama.cpp/tags
  strip: /^b/

display-name: LLaMA.cpp

provides:
  - bin/llama.cpp
  # NOTE! we do not â€œprovideâ€ convert.py. âˆµ itâ€™s too generic
  # do `tea +github.comâˆ•ggerganovâˆ•llama.cpp convert.py`

platforms:
  - linux
  - darwin/aarch64
  # Illegal instruction: 4 on darwin/x86-64

dependencies:
  pkgx.sh: ^1

build:
  dependencies:
    gnu.org/coreutils: '*'
    git-scm.org: '*'
    curl.se: '*'
    python.org: ~3.11
  env:
    VIRTUAL_ENV: ${{prefix}}/venv
    CC: clang
    CXX: clang++
    LD: clang
  script:
    # segfaults on some GHA runners
    - run: |
        sed -i.bak -e's/\(MK_.* -march=native -mtune=native\)/#\1/g' Makefile
        rm Makefile.bak
      if: linux/x86-64

    # this commit breaks linux/aarch64 - fixed in 1732
    - run: curl -LSs 'https://github.com/ggerganov/llama.cpp/pull/4630/commits/42f5246effafddcf87d67656b58e95030f4bc454.patch' | patch -p1 -R
      if: '>=1705<1732'

    - make --jobs {{hw.concurrency}}

    - |
      install -D main {{prefix}}/bin/llama.cpp
      install -D props/entrypoint.sh {{prefix}}/entrypoint.sh
      install -D ggml-metal.metal {{prefix}}/bin/ggml-metal.metal

    - |
      mkdir -p {{prefix}}/share
      mv prompts {{prefix}}/share

    - install -D convert.py $VIRTUAL_ENV/bin/convert.py

    - |
      bkpyvenv stage {{prefix}} {{version}}
      $VIRTUAL_ENV/bin/pip install -r requirements.txt
      bkpyvenv seal {{prefix}} convert.py

test:
  llama.cpp --version
  # broke in 2453, difficult to filter due to calver tweaks
  # llama.cpp --help
  # ^^ testing more than this requires downloading the models ðŸ˜¬

entrypoint: ./entrypoint.sh
