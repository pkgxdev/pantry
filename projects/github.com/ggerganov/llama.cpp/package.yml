distributable:
  url: https://github.com/ggerganov/llama.cpp/archive/refs/tags/b{{version.raw}}.tar.gz
  strip-components: 1

versions:
  github: ggerganov/llama.cpp/tags
  strip: /^b/

display-name: LLaMA.cpp

provides:
  - bin/llama-cli
  - bin/llama.cpp
  # NOTE! we do not “provide” convert.py. ∵ it’s too generic
  # do `pkgx +github.com∕ggerganov∕llama.cpp convert.py`

platforms:
  - linux
  - darwin/aarch64
  # Illegal instruction: 4 on darwin/x86-64

dependencies:
  pkgx.sh: ^1
  linux:
    gnu.org/gcc: '*' # clang doesn't provide omp.h, and we need libstdc++

build:
  dependencies:
    gnu.org/coreutils: '*'
    git-scm.org: '*'
    curl.se: '*'
    python.org: ~3.11
    cmake.org: 3
    linux/aarch64:
      kernel.org/linux-headers: '*' # hwcap.h
  env:
    VIRTUAL_ENV: ${{prefix}}/venv
  script:
    # segfaults on some GHA runners
    - run: sed -i -e's/\(MK_.* -march=native -mtune=native\)/#\1/g' Makefile
      if: linux/x86-64

    # this commit breaks linux/aarch64 - fixed in 1732
    - run: curl -LSs 'https://github.com/ggerganov/llama.cpp/pull/4630/commits/42f5246effafddcf87d67656b58e95030f4bc454.patch' | patch -p1 -R
      if: '>=1705<1732'
    # -mcpu=native doesn't work on our docker builders
    - run: sed -i -e's/-mcpu=native/-mcpu=generic/g' CMakeLists.txt
      working-directory: ggml/src/ggml-cpu
      if: linux/aarch64

    - run: sed -i -f $PROP ggml.c
      if: linux/aarch64
      working-directory: ggml/src
      prop: |
        /#include <syscall.h>/a\
        // missing include for linux/aarch64 -- HWCAP2_I8MM\\
        #include <asm/hwcap.h>

    - run: make --jobs {{hw.concurrency}}
      if: <4242
    # *** The Makefile build is deprecated. Use the CMake build instead.
    # For more details, see https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md.
    # Stop.
    - run:
        - cmake -B build $CMAKE_ARGS
        - cmake --build build --config Release
        - cmake --install build --prefix {{prefix}}
      if: '>=4242'

    # https://github.com/ggerganov/llama.cpp/blob/master/examples/deprecation-warning/README.md
    - |
      if test -f llama-cli; then
        install -D llama-cli {{prefix}}/bin/llama-cli
        # legacy name
        ln -s llama-cli {{prefix}}/bin/llama.cpp
      elif test -f main; then
        install -D main {{prefix}}/bin/llama.cpp
        # new name
        ln -s llama.cpp {{prefix}}/bin/llama-cli
      elif test -f {{prefix}}/bin/llama-cli; then
        ln -s llama-cli {{prefix}}/bin/llama.cpp
      else
        echo "No binary found"
        exit 1
      fi

    - install -D props/entrypoint.sh {{prefix}}/entrypoint.sh

    - |
      if test -f ggml-metal.metal; then
        install -D ggml-metal.metal {{prefix}}/bin/ggml-metal.metal
      elif test -f ggml/src/ggml-metal.metal; then
        install -D ggml/src/ggml-metal.metal {{prefix}}/bin/ggml-metal.metal
      elif test -f ggml/src/ggml-metal/ggml-metal.metal; then
        install -D ggml/src/ggml-metal/ggml-metal.metal {{prefix}}/bin/ggml-metal.metal
      else
        echo "No ggml-metal.metal found"
        exit 1
      fi

    - run:
        - mkdir -p {{prefix}}/share
        - mv prompts {{prefix}}/share
      if: <4242

    - |
      if test -f convert.py; then
        install -D convert.py $VIRTUAL_ENV/bin/convert.py
      elif test -f examples/convert-legacy-llama.py; then
        install -D examples/convert-legacy-llama.py $VIRTUAL_ENV/bin/convert.py
      elif test -f examples/convert_legacy_llama.py; then
        install -D examples/convert_legacy_llama.py $VIRTUAL_ENV/bin/convert.py
      else
        echo "No convert.py found"
        false
      fi

    - bkpyvenv stage {{prefix}} {{version}}
    - $VIRTUAL_ENV/bin/pip install -r requirements.txt
    - bkpyvenv seal {{prefix}} convert.py

test:
  # broke in v2453
  - run: llama.cpp --help
    if: <2453
  - run: llama.cpp --version
    if: '>=2453<3368'
  # llama.cpp deprecated in v3368
  - run: llama-cli --version
    if: '>=3368'
  # ^^ testing more than this requires downloading the models 😬

entrypoint: ./entrypoint.sh
